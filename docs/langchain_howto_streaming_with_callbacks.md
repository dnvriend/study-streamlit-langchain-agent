# Streaming with callbacks

This document explains how to achieve streaming-like behavior with Amazon Bedrock in LangChain, focusing on the use of callback handlers.

## Key Concepts

Unlike some other LLMs (e.g., OpenAI, Google Gemini), the `langchain-aws` integration with Amazon Bedrock, specifically when using `ChatBedrockConverse` or `ChatBedrock` with an `AgentExecutor`, doesn't natively support the `agent_executor.stream()` method for direct streaming of intermediate steps. You won't get an iterator of individual chunks as you might expect. Instead, you receive a single chunk containing the complete response.

To achieve a streaming effect, where you see the output progressively, you need to implement a custom `CallbackHandler`. This handler intercepts events during the agent's execution, including the arrival of new tokens from the LLM.  This approach mirrors how the `StreamlitCallbackHandler` works within the Streamlit framework.

The core of the solution lies in overriding methods of the `BaseCallbackHandler` class, particularly:

*   `on_llm_start`: Called when the LLM starts processing.  Useful for initial messages like "Thinking...".
*   `on_llm_new_token`:  This is the crucial method for streaming. It's called whenever a new token is generated by the LLM.  You can access the token and print it immediately, creating the streaming effect.
*   `on_llm_end`: Called when the LLM finishes.
*   `on_llm_error`: Called if the LLM encounters an error.
*   `on_tool_start`, `on_tool_end`, `on_tool_error`:  These handle events related to tool usage within the agent.  You can use them to display information about which tool is being used and its output.
*   `on_agent_finish`: Called when the agent completes its task.

**Important Note:** The `token` argument in `on_llm_new_token` is a dictionary, not just a string. You need to check the 'type' key (e.g., 'text', 'tool_result') and extract the content accordingly.

To make the callback handler work, you must pass it to the `invoke` method of the `AgentExecutor` within a configuration dictionary.  It looks like this:

```python
callback_handler = ConsoleCallbackHandler()
result = agent_executor.invoke({"input": user_input}, {"callbacks": [callback_handler]})
```

You *cannot* simply pass the callback handler as a separate argument; it *must* be inside the configuration dictionary under the `callbacks` key:

```python
result = agent_executor.invoke({"input": user_input}, {"callbacks": [callback_handler]})   
```


## Code


```python
from claude_bedrock import get_agent, MODEL_SONNET_37
from langchain.memory import ConversationBufferMemory
from langchain_core.callbacks import BaseCallbackHandler

class ConsoleCallbackHandler(BaseCallbackHandler):
    """Callback Handler that prints to the console in a structured format."""

    def on_llm_start(self, serialized: dict, prompts: list, **kwargs):
        """Handle the start of an LLM thought."""
        print("• Thinking...")
        print("  LLM: ", end="")

    def on_llm_new_token(self, token: str, **kwargs):
        """Handle new tokens from the LLM."""
        result = ""
        for item in token:
            if item.get('type') and item['type'] == 'text':
                result += item['text']
            elif item.get('type') and item['type'] == 'tool_result':
                result += item['text']
            elif item.get('type'):
                result += f"Unknown token type: {item.get('type')} and item='{item}'"
        print(result, end="", flush=True)

    def on_llm_end(self, response, **kwargs):
        """Handle the end of an LLM thought."""
        print()  # Move to the next line

    def on_llm_error(self, error, **kwargs):
        """Handle LLM errors."""
        print(f"  LLM error: {error}")

    def on_tool_start(self, serialized: dict, input_str: str, **kwargs):
        """Handle the start of a tool execution."""
        tool_name = serialized["name"]
        print(f"• Using tool:\n{tool_name} with input: {input_str}\n")

    def on_tool_end(self, output, **kwargs):
        """Handle the end of a tool execution."""
        print(f"    Tool output:\n{output}")

    def on_tool_error(self, error, **kwargs):
        """Handle tool errors."""
        print(f"    Tool error:\n{error}")

    def on_agent_finish(self, finish, **kwargs):
        """Handle the final output of the agent."""
        pass
        # result = finish.return_values
        # final_output = "\n".join(item['text'] for item in result["output"])
        # print(f"• Final answer: {final_output}")

import warnings
from langchain._api.deprecation import LangChainDeprecationWarning
warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
agent_executor = get_agent(option=MODEL_SONNET_37, memory=memory, streaming=True, converse_chat=True, thinking=False)

def run_agent_example(query) -> dict:
    print(f"\n--- Running agent with query: '{query}' ---")
    result = agent_executor.invoke({"input": query})
    return result

# Convert to interactive chatbot
print("\n=== Interactive Agent Chatbot ===")
print("Type 'exit', 'quit', or 'bye' to end the conversation.\n")

while True:
    user_input = input("You: ")
    
    # Check for exit commands
    if user_input.lower() in ["exit", "quit", "bye"]:
        print("Goodbye!")
        break
        
    # Process the user input through the agent
    try:
        callback_handler = ConsoleCallbackHandler()
        result = agent_executor.invoke({"input": user_input}, {"callbacks": [callback_handler]})   
        print()         
    except Exception as e:
        print(f"\nError: {str(e)}")
```